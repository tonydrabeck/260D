{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy (from -r requirements.txt (line 1))\n",
      "  Downloading numpy-2.1.3-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch==0.4.1.post2 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1)\n",
      "ERROR: No matching distribution found for torch==0.4.1.post2\n"
     ]
    }
   ],
   "source": [
    "#pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import argparse\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import ssl \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from Cutout.util.misc import CSVLogger\n",
    "from Cutout.util.cutout import Cutout\n",
    "\n",
    "from Cutout.model.resnet import ResNet18\n",
    "from Cutout.model.wide_resnet import WideResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format time for printing purposes\n",
    "def get_hms(seconds):\n",
    "    m, s = divmod(seconds, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "\n",
    "    return h, m, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce Gaussian noise to noise_percentage of image pixels\n",
    "def noisy(image, noise_percentage, noise_std):\n",
    "    row, col, ch = image.shape\n",
    "    num_corrupt = int(np.floor(noise_percentage * row * col / 100))\n",
    "\n",
    "    # Randomly choose pixels to add noise to\n",
    "    xy_coords = np.random.choice(row * col, num_corrupt, replace=False)\n",
    "    chan_coords = np.random.choice(ch, num_corrupt, replace=True)\n",
    "    xy_coords = np.unravel_index(xy_coords, (row, col))\n",
    "\n",
    "    out = np.copy(image)\n",
    "\n",
    "    mean = 120\n",
    "\n",
    "    # Add randomly generated Gaussian noise to pixels\n",
    "    for coord in range(num_corrupt):\n",
    "        noise = np.random.normal(mean, noise_std, 1)\n",
    "        out[xy_coords[0][coord], xy_coords[1][coord],\n",
    "            chan_coords[coord]] += noise\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define args \n",
    "args = {\n",
    "    'dataset': 'cifar10', \n",
    "    'model': 'resnet18', \n",
    "    'batch_size': 128, \n",
    "    'epochs': 10, \n",
    "    'learning_rate': 0.1, \n",
    "    'data_augmentation': True, \n",
    "    'cutout': False, \n",
    "    'cuda':True,\n",
    "    'n_holes': 1, \n",
    "    'length': 16, \n",
    "    'no_cuda': False, \n",
    "    'seed': 1, \n",
    "    'sorting_file': 'none', \n",
    "    'remove_n': 0, \n",
    "    'keep_lowest_n': 0, \n",
    "    'remove_subsample': 0, \n",
    "    'noise_percent_labels': 0, \n",
    "    'noise_percent_pixels': 0, \n",
    "    'noise_std_pixels': 0, \n",
    "    'optimizer': 'sgd', \n",
    "    'input_dir': 'cifar10_results/', \n",
    "    'output_dir': 'cifar10_results',\n",
    "    'data_augmentation': False,\n",
    " }\n",
    "\n",
    "\n",
    "save_fname = '__'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model for one epoch\n",
    "#\n",
    "# example_stats: dictionary containing statistics accumulated over every presentation of example\n",
    "#\n",
    "def train(args, model, device, trainset, model_optimizer, epoch,\n",
    "          example_stats):\n",
    "    train_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Get permutation to shuffle trainset\n",
    "    trainset_permutation_inds = npr.permutation(\n",
    "        np.arange(len(trainset.targets)))\n",
    "\n",
    "    print('\\n=> Training Epoch #%d' % (epoch))\n",
    "\n",
    "    batch_size = args['batch_size']\n",
    "    for batch_idx, batch_start_ind in enumerate(\n",
    "            range(0, len(trainset.targets), batch_size)):\n",
    "\n",
    "        # Get trainset indices for batch\n",
    "        batch_inds = trainset_permutation_inds[batch_start_ind:\n",
    "                                               batch_start_ind + batch_size]\n",
    "\n",
    "        # Get batch inputs and targets, transform them appropriately\n",
    "        transformed_trainset = []\n",
    "        for ind in batch_inds:\n",
    "            transformed_trainset.append(trainset.__getitem__(ind)[0])\n",
    "        inputs = torch.stack(transformed_trainset)\n",
    "        targets = torch.LongTensor(\n",
    "            np.array(trainset.targets)[batch_inds].tolist())\n",
    "\n",
    "        # Map to available device\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Forward propagation, compute loss, get predictions\n",
    "        model_optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Update statistics and loss\n",
    "        acc = predicted == targets\n",
    "        for j, index in enumerate(batch_inds):\n",
    "\n",
    "            # Get index in original dataset (not sorted by forgetting)\n",
    "            index_in_original_dataset = train_indx[index]\n",
    "\n",
    "            # Compute missclassification margin\n",
    "            output_correct_class = outputs.data[j, targets[j].item()]\n",
    "            sorted_output, _ = torch.sort(outputs.data[j, :])\n",
    "            if acc[j]:\n",
    "                # Example classified correctly, highest incorrect class is 2nd largest output\n",
    "                output_highest_incorrect_class = sorted_output[-2]\n",
    "            else:\n",
    "                # Example misclassified, highest incorrect class is max output\n",
    "                output_highest_incorrect_class = sorted_output[-1]\n",
    "            margin = output_correct_class.item(\n",
    "            ) - output_highest_incorrect_class.item()\n",
    "\n",
    "            # Add the statistics of the current training example to dictionary\n",
    "            index_stats = example_stats.get(index_in_original_dataset,\n",
    "                                            [[], [], []])\n",
    "            index_stats[0].append(loss[j].item())\n",
    "            index_stats[1].append(acc[j].sum().item())\n",
    "            index_stats[2].append(margin)\n",
    "            example_stats[index_in_original_dataset] = index_stats\n",
    "\n",
    "        # Update loss, backward propagate, update optimizer\n",
    "        loss = loss.mean()\n",
    "        train_loss += loss.item()\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "        loss.backward()\n",
    "        model_optimizer.step()\n",
    "\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(\n",
    "            '| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%' %\n",
    "            (epoch, args['epochs'], batch_idx + 1,\n",
    "             (len(trainset) // batch_size) + 1, loss.item(),\n",
    "             100. * correct.item() / total))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Add training accuracy to dict\n",
    "        index_stats = example_stats.get('train', [[], []])\n",
    "        index_stats[1].append(100. * correct.item() / float(total))\n",
    "        example_stats['train'] = index_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model predictions on heldout test data\n",
    "#\n",
    "# example_stats: dictionary containing statistics accumulated over every presentation of example\n",
    "#\n",
    "def test(epoch, model, device, example_stats):\n",
    "    global best_acc\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    test_batch_size = 32\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for batch_idx, batch_start_ind in enumerate(\n",
    "            range(0, len(test_dataset.targets), test_batch_size)):\n",
    "\n",
    "        # Get batch inputs and targets\n",
    "        transformed_testset = []\n",
    "        for ind in range(\n",
    "                batch_start_ind,\n",
    "                min(\n",
    "                    len(test_dataset.targets),\n",
    "                    batch_start_ind + test_batch_size)):\n",
    "            transformed_testset.append(test_dataset.__getitem__(ind)[0])\n",
    "        inputs = torch.stack(transformed_testset)\n",
    "        targets = torch.LongTensor(\n",
    "            np.array(\n",
    "                test_dataset.targets)[batch_start_ind:batch_start_ind +\n",
    "                                          test_batch_size].tolist())\n",
    "\n",
    "        # Map to available device\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Forward propagation, compute loss, get predictions\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss = loss.mean()\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "    # Add test accuracy to dict\n",
    "    acc = 100. * correct.item() / total\n",
    "    index_stats = example_stats.get('test', [[], []])\n",
    "    index_stats[1].append(100. * correct.item() / float(total))\n",
    "    example_stats['test'] = index_stats\n",
    "    print(\"\\n| Validation Epoch #%d\\t\\t\\tLoss: %.4f Acc@1: %.2f%%\" %\n",
    "          (epoch, loss.item(), acc))\n",
    "\n",
    "    # Save checkpoint when best model\n",
    "    if acc > best_acc:\n",
    "        print('| Saving Best model...\\t\\t\\tTop1 = %.2f%%' % (acc))\n",
    "        state = {\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        save_point = os.path.join(args['output_dir'], 'checkpoint', args['dataset'])\n",
    "        os.makedirs(save_point, exist_ok=True)\n",
    "        torch.save(state, os.path.join(save_point, save_fname + '.t7'))\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:34<00:00, 4.98MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\cifar-10-python.tar.gz to data\n",
      "Files already downloaded and verified\n",
      "Trainset of 50000 examples\n"
     ]
    }
   ],
   "source": [
    "#prevents ssl certificate errors \n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Image Preprocessing\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "    std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "# define train transform \n",
    "train_transform = transforms.Compose([])\n",
    "if args['data_augmentation']:\n",
    "    train_transform.transforms.append(transforms.RandomCrop(32, padding=4))\n",
    "    train_transform.transforms.append(transforms.RandomHorizontalFlip())\n",
    "train_transform.transforms.append(transforms.ToTensor())\n",
    "train_transform.transforms.append(normalize)\n",
    "if args['cutout']:\n",
    "    train_transform.transforms.append(\n",
    "        Cutout(n_holes=args.n_holes, length=args.length))\n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "# Check for output dir \n",
    "os.makedirs(args['output_dir'], exist_ok=True)\n",
    "\n",
    "# Load the appropriate train and test datasets\n",
    "if args['dataset'] == 'cifar10':\n",
    "    num_classes = 10\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root='data',\n",
    "        train=True,\n",
    "        transform=train_transform,\n",
    "        download=True)\n",
    "\n",
    "    test_dataset = datasets.CIFAR10(\n",
    "        root='data',\n",
    "        train=False,\n",
    "        transform=test_transform,\n",
    "        download=True)\n",
    "elif args['dataset'] == 'cifar100':\n",
    "    num_classes = 100\n",
    "    train_dataset = datasets.CIFAR100(\n",
    "        root='data',\n",
    "        train=True,\n",
    "        transform=train_transform,\n",
    "        download=True)\n",
    "\n",
    "    test_dataset = datasets.CIFAR100(\n",
    "        root='data',\n",
    "        train=False,\n",
    "        transform=test_transform,\n",
    "        download=True)\n",
    "\n",
    "train_indx = npr.permutation(np.arange(len(\n",
    "        train_dataset.targets)))[:len(train_dataset.targets) -\n",
    "                                      0]\n",
    "\n",
    "train_dataset.data = train_dataset.data[train_indx, :, :, :]\n",
    "train_dataset.targets = np.array(\n",
    "    train_dataset.targets)[train_indx].tolist()\n",
    "\n",
    "print('Trainset of ' + str(len(train_dataset.targets)) + ' examples')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tony\\miniconda3\\envs\\HW3\\Lib\\site-packages\\torch\\nn\\_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "#device settings\n",
    "args['cuda'] = args['cuda'] and torch.cuda.is_available()\n",
    "use_cuda = args['cuda']\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "cudnn.benchmark = True  # Should make training go faster for large models\n",
    "\n",
    "# Set random seed for initialization\n",
    "torch.manual_seed(args['seed'])\n",
    "if args['cuda']:\n",
    "    torch.cuda.manual_seed(args['seed'])\n",
    "npr.seed(args['seed'])\n",
    "\n",
    "# Image Preprocessing\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "    std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "# Setup model\n",
    "if args['model'] == 'resnet18':\n",
    "    model = ResNet18(num_classes=num_classes)\n",
    "elif args['model'] == 'wideresnet':\n",
    "        model = WideResNet(\n",
    "            depth=28, num_classes=num_classes, widen_factor=10, dropRate=0.3)\n",
    "else:\n",
    "    print(\n",
    "        'Specified model not recognized. Options are: resnet18 and wideresnet')\n",
    "    \n",
    "# Setup loss\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "criterion.__init__(reduce=False)\n",
    "\n",
    "# Setup optimizer\n",
    "if args['optimizer'] == 'adam':\n",
    "    model_optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "elif args['optimizer'] == 'sgd':\n",
    "    model_optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=args['learning_rate'],\n",
    "        momentum=0.9,\n",
    "        nesterov=True,\n",
    "        weight_decay=5e-4)\n",
    "    scheduler = MultiStepLR(\n",
    "        model_optimizer, milestones=[60, 120, 160], gamma=0.2)\n",
    "else:\n",
    "    print('Specified optimizer not recognized. Options are: adam and sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=> Training Epoch #0\n",
      "| Epoch [  0/ 10] Iter[391/391]\t\tLoss: 1.7074 Acc@1: 28.998%\n",
      "| Validation Epoch #0\t\t\tLoss: 1.8637 Acc@1: 37.95%\n",
      "| Saving Best model...\t\t\tTop1 = 37.95%\n",
      "| Elapsed time : 0:03:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tony\\miniconda3\\envs\\HW3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=> Training Epoch #1\n",
      "| Epoch [  1/ 10] Iter[391/391]\t\tLoss: 1.2506 Acc@1: 46.856%\n",
      "| Validation Epoch #1\t\t\tLoss: 1.2659 Acc@1: 51.25%\n",
      "| Saving Best model...\t\t\tTop1 = 51.25%\n",
      "| Elapsed time : 0:06:16\n",
      "\n",
      "=> Training Epoch #2\n",
      "| Epoch [  2/ 10] Iter[391/391]\t\tLoss: 1.0101 Acc@1: 58.726%\n",
      "| Validation Epoch #2\t\t\tLoss: 1.1088 Acc@1: 56.04%\n",
      "| Saving Best model...\t\t\tTop1 = 56.04%\n",
      "| Elapsed time : 0:09:33\n",
      "\n",
      "=> Training Epoch #3\n",
      "| Epoch [  3/ 10] Iter[391/391]\t\tLoss: 0.9436 Acc@1: 67.298%\n",
      "| Validation Epoch #3\t\t\tLoss: 1.2252 Acc@1: 60.06%\n",
      "| Saving Best model...\t\t\tTop1 = 60.06%\n",
      "| Elapsed time : 0:12:41\n",
      "\n",
      "=> Training Epoch #4\n",
      "| Epoch [  4/ 10] Iter[391/391]\t\tLoss: 0.6884 Acc@1: 72.992%\n",
      "| Validation Epoch #4\t\t\tLoss: 0.9292 Acc@1: 69.70%\n",
      "| Saving Best model...\t\t\tTop1 = 69.70%\n",
      "| Elapsed time : 0:15:50\n",
      "\n",
      "=> Training Epoch #5\n",
      "| Epoch [  5/ 10] Iter[391/391]\t\tLoss: 0.5614 Acc@1: 77.270%\n",
      "| Validation Epoch #5\t\t\tLoss: 0.7216 Acc@1: 71.08%\n",
      "| Saving Best model...\t\t\tTop1 = 71.08%\n",
      "| Elapsed time : 0:19:01\n",
      "\n",
      "=> Training Epoch #6\n",
      "| Epoch [  6/ 10] Iter[391/391]\t\tLoss: 0.4434 Acc@1: 81.026%\n",
      "| Validation Epoch #6\t\t\tLoss: 0.5579 Acc@1: 74.29%\n",
      "| Saving Best model...\t\t\tTop1 = 74.29%\n",
      "| Elapsed time : 0:22:09\n",
      "\n",
      "=> Training Epoch #7\n",
      "| Epoch [  7/ 10] Iter[391/391]\t\tLoss: 0.5145 Acc@1: 83.436%\n",
      "| Validation Epoch #7\t\t\tLoss: 0.5519 Acc@1: 74.31%\n",
      "| Saving Best model...\t\t\tTop1 = 74.31%\n",
      "| Elapsed time : 0:25:26\n",
      "\n",
      "=> Training Epoch #8\n",
      "| Epoch [  8/ 10] Iter[391/391]\t\tLoss: 0.6656 Acc@1: 85.514%\n",
      "| Validation Epoch #8\t\t\tLoss: 0.3551 Acc@1: 77.37%\n",
      "| Saving Best model...\t\t\tTop1 = 77.37%\n",
      "| Elapsed time : 0:28:34\n",
      "\n",
      "=> Training Epoch #9\n",
      "| Epoch [  9/ 10] Iter[391/391]\t\tLoss: 0.5056 Acc@1: 87.148%\n",
      "| Validation Epoch #9\t\t\tLoss: 0.3931 Acc@1: 79.00%\n",
      "| Saving Best model...\t\t\tTop1 = 79.00%\n",
      "| Elapsed time : 0:31:46\n"
     ]
    }
   ],
   "source": [
    "example_stats = {}\n",
    "\n",
    "best_acc = 0\n",
    "elapsed_time = 0\n",
    "for epoch in range(args['epochs']):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train(args, model, device, train_dataset, model_optimizer, epoch,\n",
    "          example_stats)\n",
    "    test(epoch, model, device, example_stats)\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    elapsed_time += epoch_time\n",
    "    print('| Elapsed time : %d:%02d:%02d' % (get_hms(elapsed_time)))\n",
    "\n",
    "    # Update optimizer step\n",
    "    if args['optimizer'] == 'sgd':\n",
    "        scheduler.step(epoch)\n",
    "\n",
    "    # Save the stats dictionary\n",
    "    fname = os.path.join(args['output_dir'], save_fname)\n",
    "    with open(fname + \"__stats_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(example_stats, f)\n",
    "\n",
    "    # Log the best train and test accuracy so far\n",
    "    with open(fname + \"__best_acc.txt\", \"w\") as f:\n",
    "        f.write('train test \\n')\n",
    "        f.write(str(max(example_stats['train'][1])))\n",
    "        f.write(' ')\n",
    "        f.write(str(max(example_stats['test'][1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 0,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 5,\n",
       " 9,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 7,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 7,\n",
       " 9,\n",
       " 2,\n",
       " 4,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 1,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 6,\n",
       " 9,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 5,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 9,\n",
       " 0,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 9,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 0,\n",
       " 7,\n",
       " 9,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 6,\n",
       " 3,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 7,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 9,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 2,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 6,\n",
       " 9,\n",
       " 0,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 2,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 9,\n",
       " 0,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 2,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 6,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 2,\n",
       " 5,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 7,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 8,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 9,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 9,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 3,\n",
       " 7,\n",
       " 0,\n",
       " 5,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 9,\n",
       " 3,\n",
       " 7,\n",
       " 0,\n",
       " 6,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 9,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 2,\n",
       " 7,\n",
       " 0,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 9,\n",
       " 2,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 7,\n",
       " 6,\n",
       " 3,\n",
       " 1,\n",
       " 9,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 3,\n",
       " 9,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 9,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 9,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 7,\n",
       " 2,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 7,\n",
       " 5,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 9,\n",
       " 2,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 9,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 8,\n",
       " 4,\n",
       " 9,\n",
       " 9,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 9,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 9,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 9,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 2,\n",
       " 7,\n",
       " 9,\n",
       " 1,\n",
       " 9,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 3,\n",
       " 9,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 9,\n",
       " 2,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 9,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 7,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 7,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 7,\n",
       " 1,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 2,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 7,\n",
       " 0,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 7,\n",
       " 3,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 9,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " ...]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HW3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
